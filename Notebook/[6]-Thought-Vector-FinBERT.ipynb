{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10753958,"sourceType":"datasetVersion","datasetId":6669828},{"sourceId":10801699,"sourceType":"datasetVersion","datasetId":6704262},{"sourceId":10819892,"sourceType":"datasetVersion","datasetId":6717789},{"sourceId":10820224,"sourceType":"datasetVersion","datasetId":6718001},{"sourceId":265224,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":226893,"modelId":248676},{"sourceId":265851,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":227454,"modelId":249235},{"sourceId":266045,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":227633,"modelId":249404}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom tqdm import tqdm\n","metadata":{"id":"79LKjklvc3gB","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T07:40:48.391787Z","iopub.execute_input":"2025-02-22T07:40:48.392095Z","iopub.status.idle":"2025-02-22T07:40:51.823362Z","shell.execute_reply.started":"2025-02-22T07:40:48.392066Z","shell.execute_reply":"2025-02-22T07:40:51.822740Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read Data and Build Autoencoder Model","metadata":{}},{"cell_type":"markdown","source":"### EPOCH: 50","metadata":{}},{"cell_type":"code","source":"# 🔹 Load data dari .npz\ndata = np.load(\"/kaggle/input/finbert-a-embeddings-sector/finbert_embeddings_all_sectors.npz\", allow_pickle=True)\nembeddings = data[\"Embedding\"]  # Shape: (94032, 25, 768)\nprint(embeddings.shape)  # Pastikan outputnya benar\n\n# 🔹 Dataset Custom (IterableDataset)\nclass EmbeddingDataset(IterableDataset):\n    def __init__(self, embeddings):\n        super().__init__()\n        self.embeddings = embeddings\n\n    def __iter__(self):\n        for emb in self.embeddings:\n            yield torch.tensor(emb, dtype=torch.float32)  # Konversi ke tensor saat iterasi\n\n# 🔹 Load Data dengan DataLoader\ndataset = EmbeddingDataset(embeddings)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # shuffle=False karena IterableDataset\n\n# 🔹 Model Autoencoder\nclass Seq2SeqAutoencoder(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, num_layers=4, dropout=0.3):\n        super(Seq2SeqAutoencoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        # Encoder\n        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        # Decoder\n        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, input_dim)  # Output layer ke dimensi asli\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n\n        # Encoder\n        _, (hidden, cell) = self.encoder(x)\n\n        # Decoder\n        decoder_input = torch.zeros(batch_size, 1, self.hidden_dim).to(x.device)  # Input pertama (zero vector)\n        outputs = []\n\n        for _ in range(seq_len):\n            out, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n            pred = self.fc_out(out)\n            outputs.append(pred)\n            decoder_input = pred\n\n        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_len, input_dim]\n        return outputs, hidden  # Hidden = Thought Vector\n\n# 🔹 Inisialisasi Model, Optimizer, Loss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Seq2SeqAutoencoder().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# 🔹 Training Loop\nepochs = 50\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        batch = batch.to(device)  # Pindahkan ke GPU jika tersedia\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n\ntorch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T22:30:49.690751Z","iopub.execute_input":"2025-02-21T22:30:49.691071Z","iopub.status.idle":"2025-02-22T04:19:57.929869Z","shell.execute_reply.started":"2025-02-21T22:30:49.691039Z","shell.execute_reply":"2025-02-22T04:19:57.929070Z"},"scrolled":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Update Dataset","metadata":{}},{"cell_type":"code","source":"# 🔹 Load data dari .npz\ndata = np.load(\"/kaggle/input/finbert-embeddings-sectoral/finbert_embeddings_all_sectors.npz\", allow_pickle=True)\nembeddings = data[\"Embedding\"]  # Shape: (94032, 25, 768)\nprint(embeddings.shape)  # Pastikan outputnya benar\n\n# 🔹 Dataset Custom (IterableDataset)\nclass EmbeddingDataset(IterableDataset):\n    def __init__(self, embeddings):\n        super().__init__()\n        self.embeddings = embeddings\n\n    def __iter__(self):\n        for emb in self.embeddings:\n            yield torch.tensor(emb, dtype=torch.float32)  # Konversi ke tensor saat iterasi\n\n# 🔹 Load Data dengan DataLoader\ndataset = EmbeddingDataset(embeddings)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # shuffle=False karena IterableDataset\n\n# 🔹 Model Autoencoder\nclass Seq2SeqAutoencoder(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, num_layers=4, dropout=0.3):\n        super(Seq2SeqAutoencoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        # Encoder\n        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        # Decoder\n        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, input_dim)  # Output layer ke dimensi asli\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n\n        # Encoder\n        _, (hidden, cell) = self.encoder(x)\n\n        # Decoder\n        decoder_input = torch.zeros(batch_size, 1, self.hidden_dim).to(x.device)  # Input pertama (zero vector)\n        outputs = []\n\n        for _ in range(seq_len):\n            out, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n            pred = self.fc_out(out)\n            outputs.append(pred)\n            decoder_input = pred\n\n        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_len, input_dim]\n        return outputs, hidden  # Hidden = Thought Vector\n\n# 🔹 Inisialisasi Model, Optimizer, Loss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Seq2SeqAutoencoder().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:05:09.692723Z","iopub.execute_input":"2025-02-23T05:05:09.693029Z","iopub.status.idle":"2025-02-23T05:05:22.082127Z","shell.execute_reply.started":"2025-02-23T05:05:09.693006Z","shell.execute_reply":"2025-02-23T05:05:22.081457Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 50-60","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 50\nadditional_epochs = 20  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/input/finbert_autoencoder_lstm.pth/pytorch/default/1/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T08:09:22.034643Z","iopub.execute_input":"2025-02-22T08:09:22.035024Z","execution_failed":"2025-02-22T09:41:06.419Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 59 - 70","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 58\nadditional_epochs = 12  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/working/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:55:00.251278Z","iopub.execute_input":"2025-02-22T10:55:00.251766Z","iopub.status.idle":"2025-02-22T12:16:55.448973Z","shell.execute_reply.started":"2025-02-22T10:55:00.251737Z","shell.execute_reply":"2025-02-22T12:16:55.448000Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 71-90","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 70\nadditional_epochs = 20  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/working/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T12:23:36.165595Z","iopub.execute_input":"2025-02-22T12:23:36.165895Z","iopub.status.idle":"2025-02-22T14:40:14.652364Z","shell.execute_reply.started":"2025-02-22T12:23:36.165869Z","shell.execute_reply":"2025-02-22T14:40:14.651370Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 91 - 150","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 90\nadditional_epochs = 60  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/input/finbert-seq2seq/pytorch/default/1/finbert_autoencoder.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:20:27.012981Z","iopub.execute_input":"2025-02-22T15:20:27.013428Z","iopub.status.idle":"2025-02-22T22:44:31.551684Z","shell.execute_reply.started":"2025-02-22T15:20:27.013386Z","shell.execute_reply":"2025-02-22T22:44:31.550339Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 150 - 180","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 150\nadditional_epochs = 30  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/input/autoencoder/pytorch/default/1/finbert_autoencoder_lstm (1).pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T22:55:39.614469Z","iopub.execute_input":"2025-02-22T22:55:39.614972Z","execution_failed":"2025-02-23T00:49:02.091Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 166 - 180","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 166\nadditional_epochs = 14  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/working/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T00:53:49.601642Z","iopub.execute_input":"2025-02-23T00:53:49.602231Z","iopub.status.idle":"2025-02-23T02:27:23.336122Z","shell.execute_reply.started":"2025-02-23T00:53:49.602198Z","shell.execute_reply":"2025-02-23T02:27:23.335350Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 180-200","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 180\nadditional_epochs = 20  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/working/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T02:28:58.386109Z","iopub.execute_input":"2025-02-23T02:28:58.386447Z","iopub.status.idle":"2025-02-23T04:42:43.441171Z","shell.execute_reply.started":"2025-02-23T02:28:58.386424Z","shell.execute_reply":"2025-02-23T04:42:43.440015Z"},"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Melanjutkan training. EPOCH: 200 - 205","metadata":{}},{"cell_type":"code","source":"# Lanjutkan training\nepochs = 200\nadditional_epochs = 5  # Jumlah epoch tambahan\ntotal_epochs = epochs + additional_epochs  # Total epoch setelah dilanjutkan\n\n# Muat model yang sudah disimpan\nmodel.load_state_dict(torch.load(\"/kaggle/working/finbert_autoencoder_lstm.pth\", weights_only=True))\nprint(\"Model loaded. Melanjutkan training...\")\n\nfor epoch in range(epochs, total_epochs):\n    model.train()\n    epoch_loss = 0\n    batch_count = 0\n\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\"):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n\n        output, hidden = model(batch)\n        loss = criterion(output, batch)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        batch_count += 1  \n        \n    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / batch_count:.6f}\")\n\n    # Simpan model setiap epoch (opsional)\n    torch.save(model.state_dict(), \"finbert_autoencoder_lstm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:09:28.261789Z","iopub.execute_input":"2025-02-23T05:09:28.262268Z","iopub.status.idle":"2025-02-23T05:42:50.393501Z","shell.execute_reply.started":"2025-02-23T05:09:28.262242Z","shell.execute_reply":"2025-02-23T05:42:50.392491Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, IterableDataset\nimport numpy as np\nfrom tqdm import tqdm\n\n# 2️⃣ Load embedding dari file NPZ\ndf = np.load(\"/kaggle/input/finbert-embeddings-sectoral/finbert_embeddings_all_sectors.npz\", allow_pickle=True)\ndates = df[\"Only_Date\"]\ntitles = df[\"Title_Translated\"]\nsectors = df[\"Sector\"]\nembeddings = df[\"Embedding\"] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:55:25.435778Z","iopub.execute_input":"2025-02-23T05:55:25.436089Z","iopub.status.idle":"2025-02-23T05:56:16.027531Z","shell.execute_reply.started":"2025-02-23T05:55:25.436068Z","shell.execute_reply":"2025-02-23T05:56:16.026713Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initiate autoencoder model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, IterableDataset\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Seq2SeqAutoencoder(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, num_layers=4, dropout=0.3):\n        super(Seq2SeqAutoencoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        # Encoder\n        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        # Decoder\n        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, input_dim)  # Output layer ke dimensi asli\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.shape\n\n        # Encoder\n        _, (hidden, cell) = self.encoder(x)\n\n        # Decoder\n        decoder_input = torch.zeros(batch_size, 1, self.hidden_dim).to(x.device)  # Input pertama (zero vector)\n        outputs = []\n\n        for _ in range(seq_len):\n            out, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n            pred = self.fc_out(out)\n            outputs.append(pred)\n            decoder_input = pred\n\n        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_len, input_dim]\n        return outputs, hidden  # Hidden = Thought Vector\n\n# 🔹 Inisialisasi Model, Optimizer, Loss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Seq2SeqAutoencoder().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:58:50.141994Z","iopub.execute_input":"2025-02-23T05:58:50.142489Z","iopub.status.idle":"2025-02-23T05:58:53.194349Z","shell.execute_reply.started":"2025-02-23T05:58:50.142446Z","shell.execute_reply":"2025-02-23T05:58:53.193496Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transform embedding to thought vector","metadata":{}},{"cell_type":"markdown","source":"model location: /kaggle/input/autoencoder-final/pytorch/default/1/finbert_autoencoder_lstm (3).pth","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\n\n\n# Load model & pindahkan ke device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Seq2SeqAutoencoder()\nmodel.load_state_dict(torch.load(\"/kaggle/input/autoencoder-final/pytorch/default/1/finbert_autoencoder_lstm (3).pth\", map_location=device))\nmodel.to(device)\nmodel.eval()\n\n\n# 3️⃣ Inferensi dalam batch (menghindari OOM)\nBATCH_SIZE = 256 \nthought_vectors_list = []\n\nwith torch.no_grad():\n    for i in range(0, len(embeddings), BATCH_SIZE):\n        batch_embeddings = torch.tensor(embeddings[i:i+BATCH_SIZE], dtype=torch.float32).to(device)\n\n        # Ambil hanya hidden state dari model\n        _, batch_thought_vectors = model(batch_embeddings)  \n        # Ubah dimensi jadi (batch_size, 4, 768)\n        batch_thought_vectors = batch_thought_vectors.permute(1, 0, 2).cpu().numpy()\n        thought_vectors_list.append(batch_thought_vectors)\n\n\n# 4️⃣ Gabungkan semua batch\nthought_vectors = np.concatenate(thought_vectors_list, axis=0)  \n\n# 5️⃣ Simpan ke dataframe\ndf_thought = pd.DataFrame({\n    \"date\": dates,\n    \"title\": titles,\n    \"sector\": sectors,\n    \"thought_vector\": list(thought_vectors) \n})\n\nprint(\"Jumlah Data:\", len(df_thought))\ndf_thought.to_pickle(\"thought_vectors_finbert_sectoral.pkl\")\nprint(\"✅ Inferensi selesai! Thought vectors disimpan dalam dataframe.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T05:59:51.142689Z","iopub.execute_input":"2025-02-23T05:59:51.143136Z","iopub.status.idle":"2025-02-23T06:01:14.464677Z","shell.execute_reply.started":"2025-02-23T05:59:51.143096Z","shell.execute_reply":"2025-02-23T06:01:14.463902Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_thought['thought_vector'][94000].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:04:37.957895Z","iopub.execute_input":"2025-02-23T06:04:37.958201Z","iopub.status.idle":"2025-02-23T06:04:37.981937Z","shell.execute_reply.started":"2025-02-23T06:04:37.958177Z","shell.execute_reply":"2025-02-23T06:04:37.981018Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save thought vector to Kaggle Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\n# ✅ Define dataset variables\nDATASET_FOLDER = \"/kaggle/working/thought-vector-sectoral\"  \nDATASET_NAME = \"thought-vector-sectoral\"  \nUSERNAME = \"anggraininovi\"\n\n# ✅ Create dataset folder\nos.makedirs(DATASET_FOLDER, exist_ok=True)\n\n# ✅ Move necessary files to the dataset folder\n!mv /kaggle/working/thought_vectors_finbert_sectoral.pkl \"{DATASET_FOLDER}/\"\n\n# ✅ Read Kaggle API credentials\nwith open(\"/kaggle/input/d/anggraininovi/kaggle-json/kaggle (2).json\") as f:\n    kaggle_creds = json.load(f)\n\nos.environ[\"KAGGLE_USERNAME\"] = kaggle_creds[\"username\"]\nos.environ[\"KAGGLE_KEY\"] = kaggle_creds[\"key\"]\n\n# ✅ Initialize Kaggle dataset\n!kaggle datasets init -p \"{DATASET_FOLDER}\"\n\n# ✅ Update dataset metadata\nwith open(f\"{DATASET_FOLDER}/dataset-metadata.json\") as f:\n    dataset_meta = json.load(f)\n\ndataset_meta[\"id\"] = f\"{USERNAME}/{DATASET_NAME}\"\ndataset_meta[\"title\"] = DATASET_NAME\n\nwith open(f\"{DATASET_FOLDER}/dataset-metadata.json\", \"w\") as outfile:\n    json.dump(dataset_meta, outfile)\n\n# ✅ Upload dataset to Kaggle\n!kaggle datasets create -p \"{DATASET_FOLDER}\" --dir-mode zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:06:06.770771Z","iopub.execute_input":"2025-02-23T06:06:06.771099Z","iopub.status.idle":"2025-02-23T06:06:18.963480Z","shell.execute_reply.started":"2025-02-23T06:06:06.771075Z","shell.execute_reply":"2025-02-23T06:06:18.962664Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}